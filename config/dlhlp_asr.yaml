data:
  corpus:                                 # Pass to dataloader
    # The following depends on corpus
    name: 'DLHLP'                         # Specify corpus
    path: '/data/storage/harry/test_env/DLHLP'
    train_split: ['train']
    dev_split: ['dev']
    bucketing: True
    batch_size: 32
    
  audio:                                  # Pass to audio transform
    feat_type: 'fbank'
    feat_dim:  80
    apply_cmvn: False
    delta_order: 1                        # 0: do nothing, 1: add delta, 2: add delta and accelerate
    delta_window_size: 2
    frame_length: 25 # ms
    frame_shift: 10 # ms
    ref_level_db: 20
    min_level_db: -100
    preemphasis_coeff: 0.97

  text:
    mode: 'character'                     # 'character'/'word == phone'/'subword'
    vocab_file: 'corpus/bopomo_vocab.txt'

hparas:                                   # Experiment hyper-parameters
  valid_step: 500
  max_step: 80000
  tf_start: 1.0
  tf_end: 1.0
  tf_step: 150000
  optimizer: 'Adadelta'
  lr: 1.0
  eps: 0.00000001                          # 1e-8
  lr_scheduler: 'fixed'                    # 'fixed'/'warmup'
  curriculum: 0
  val_mode: 'cer'

model:                                     # Model architecture
  ctc_weight: 0.5                          # Weight for CTC loss
  encoder:
    vgg: 1                                 # 4x reduction on time feature extraction
    vgg_freq: 1
    vgg_low_filt: -1
    module: 'LSTM'                         # 'LSTM'/'GRU'/'Transformer'
    bidirection: True
    dim: [512,512]
    dropout: [0.2,0.2]
    layer_norm: [False,False]
    proj: [True,True]                      # Linear projection + Tanh after each rnn layer
    sample_rate: [1,1]
    sample_style: 'drop'                   # 'drop'/'concat'
  attention:
    mode: 'loc'                            # 'dot'/'loc'
    dim: 300
    num_head: 1
    v_proj: False                          # if False and num_head>1, encoder state will be duplicated for each head
    temperature: 0.5                       # scaling factor for attention
    loc_kernel_size: 100                   # just for mode=='loc'
    loc_kernel_num: 10                     # just for mode=='loc'
  decoder:
    module: 'LSTM'                         # 'LSTM'/'GRU'/'Transformer'
    dim: 512
    layer: 1
    dropout: 0
